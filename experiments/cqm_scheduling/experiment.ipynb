{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbde7be8-cf6b-4297-8ae8-766d3c295603",
   "metadata": {},
   "source": [
    "# SPWD algorithm usability for the CQM hybrid solver (Improving CQMcapacity with SPWD)\n",
    "\n",
    "For the explanation and results of this experiment please refer to [this context](https://github.com/mkroczek/SPWD-experiments/tree/master/experiments/cqm_scheduling).\n",
    "\n",
    "## How to run\n",
    "1. clone https://github.com/wfcommons/pegasus-instances repository. Workflows from this repository will be used in the experiment.\n",
    "2. set PEGASUS_INSTANCES_DIR to the directory containing cloned repo\n",
    "3. remember to have DWAVE_API_TOKEN env variable to be able to use CQM\n",
    "4. remember to uncomment code \"Enable CQM for final experiment run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0266e4c1-c912-43d4-b4ae-c322e1c890ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEGASUS_INSTANCES_DIR = \"/Users/marcinkroczek/code/pegasus-instances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649123e2-5e2d-4e63-bd7a-81ba0735d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "sys.path.append(os.path.abspath(\"../../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae648b14-d4fc-4cc6-896f-ef057c3c33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from report import ExecutionReport, Solution\n",
    "from QHyper.problems.workflow_scheduling import Workflow\n",
    "from decomposition.qhyper.algorithm import WorkflowDecompositionQHyperAdapter\n",
    "from decomposition.qhyper.solver import DecomposedWorkflowSchedulingSolver, WorkflowSchedulingSolverDecorator, WorkflowSchedule\n",
    "from decomposition.qhyper.problem import WorkflowSchedulingOneHotEnhanced\n",
    "from QHyper.solvers.classical.gurobi import Gurobi\n",
    "from QHyper.solvers.quantum_annealing.dwave.cqm import CQM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcbe8c4-cdde-41e2-85b5-ac1cf6676496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentResult(ABC):\n",
    "    @abstractmethod\n",
    "    def plot(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class AlgorithmRun:\n",
    "    max_subgraph_size: int\n",
    "    decomposition_schedule: WorkflowSchedule\n",
    "    reference_schedule: WorkflowSchedule\n",
    "\n",
    "class SolverFactory(ABC):\n",
    "    def __init__(self, tasks_file, machines_file, deadline):\n",
    "        self.tasks_file = tasks_file\n",
    "        self.machines_file = machines_file\n",
    "        self.deadline = deadline\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_decomposed_solver(self, max_subgraph_size: int):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_reference_solver(self):\n",
    "        pass\n",
    "    \n",
    "class GurobiSolverFactory(SolverFactory):\n",
    "    def get_decomposed_solver(self, max_subgraph_size: int):\n",
    "        workflow = Workflow(self.tasks_file, self.machines_file, self.deadline)\n",
    "        division = WorkflowDecompositionQHyperAdapter(workflow).decompose(max_subgraph_size)\n",
    "        problems = map(lambda w: WorkflowSchedulingOneHotEnhanced(w), division.workflows)\n",
    "        solvers = map(lambda p: WorkflowSchedulingSolverDecorator(Gurobi(p)), problems)\n",
    "        return DecomposedWorkflowSchedulingSolver(list(solvers), division)\n",
    "\n",
    "    def get_reference_solver(self):\n",
    "        workflow = Workflow(self.tasks_file, self.machines_file, self.deadline)\n",
    "        return WorkflowSchedulingSolverDecorator(Gurobi(WorkflowSchedulingOneHotEnhanced(workflow)))\n",
    "\n",
    "class CQMSolverFactory(SolverFactory):\n",
    "    def __init__(self, tasks_file, machines_file, deadline, time=5):\n",
    "        super().__init__(tasks_file, machines_file, deadline)\n",
    "        # time sets a maximum execution time for the experiment on CQM. It is expressed in seconds\n",
    "        self.time=time\n",
    "\n",
    "    def get_decomposed_solver(self, max_subgraph_size: int):\n",
    "        workflow = Workflow(self.tasks_file, self.machines_file, self.deadline)\n",
    "        division = WorkflowDecompositionQHyperAdapter(workflow).decompose(max_subgraph_size)\n",
    "        problems = map(lambda w: WorkflowSchedulingOneHotEnhanced(w), division.workflows)\n",
    "        solvers = map(lambda p: WorkflowSchedulingSolverDecorator(CQM(problem=p, time=self.time)), problems)\n",
    "        return DecomposedWorkflowSchedulingSolver(list(solvers), division)\n",
    "\n",
    "    def get_reference_solver(self):\n",
    "        workflow = Workflow(self.tasks_file, self.machines_file, self.deadline)\n",
    "        return WorkflowSchedulingSolverDecorator(CQM(problem=WorkflowSchedulingOneHotEnhanced(workflow), time=self.time))\n",
    "\n",
    "\n",
    "class Experiment(ABC):\n",
    "    def __init__(self, tasks_file, machines_file, deadline):\n",
    "        self.tasks_file = tasks_file\n",
    "        self.machines_file = machines_file\n",
    "        self.deadline = deadline\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self) -> ExperimentResult:\n",
    "        pass\n",
    "\n",
    "\n",
    "class CQMDecompositionExperiment(Experiment):\n",
    "    class CQMExperimentResult(ExperimentResult):\n",
    "        def __init__(\n",
    "            self, \n",
    "            gurobi_scheduling: AlgorithmRun, \n",
    "            cqm_scheduling: AlgorithmRun,\n",
    "            tasks_file: str,\n",
    "            machines_file: str,\n",
    "            deadline: int,\n",
    "            max_subgraph_size: int\n",
    "        ):\n",
    "            self.gurobi_scheduling: AlgorithmRun = gurobi_scheduling\n",
    "            self.cqm_scheduling: AlgorithmRun = cqm_scheduling\n",
    "            self.tasks_file=tasks_file\n",
    "            self.machines_file=machines_file\n",
    "            self.deadline=deadline\n",
    "            self.max_subgraph_size=max_subgraph_size\n",
    "\n",
    "        def save_execution_report(self, save_dir: str, file_prefix: str):\n",
    "            # \n",
    "            # cqm_decomposed_report = self._get_execution_report(\n",
    "            #     solver=\"CQM\",\n",
    "            #     workflow_schedule=self.cqm_scheduling.decomposition_schedule\n",
    "            # )\n",
    "            # cqm_decomposed_report.write_json(os.path.join(save_dir, f\"{file_prefix}_cqm_decomposed.json\"))\n",
    "\n",
    "            gurobi_decomposed_report = self._get_execution_report(\n",
    "                solver=\"Gurobi\",\n",
    "                workflow_schedule=self.gurobi_scheduling.decomposition_schedule\n",
    "            )\n",
    "            gurobi_decomposed_report.write_json(os.path.join(save_dir, f\"{file_prefix}_gurobi_decomposed.json\"))\n",
    "\n",
    "            gurobi_raw_report = self._get_execution_report(\n",
    "                solver=\"Gurobi\",\n",
    "                workflow_schedule=self.gurobi_scheduling.reference_schedule\n",
    "            )\n",
    "            gurobi_raw_report.write_json(os.path.join(save_dir, f\"{file_prefix}_gurobi_raw.json\"))\n",
    "\n",
    "        \n",
    "        def _get_execution_report(self, solver: str, workflow_schedule: WorkflowSchedule):\n",
    "            return ExecutionReport(\n",
    "                workflow_file=self.tasks_file,\n",
    "                machines_file=self.machines_file,\n",
    "                deadline=self.deadline,\n",
    "                max_subgraph_size=self.max_subgraph_size,\n",
    "                solver=solver,\n",
    "                solution=Solution.from_workflow_schedule(workflow_schedule)\n",
    "            )\n",
    "        \n",
    "        def plot(self):\n",
    "            pass\n",
    "        \n",
    "    def __init__(self, tasks_file, machines_file, max_subgraph_size: int):\n",
    "        super().__init__(tasks_file, machines_file, deadline_as_cpv(tasks_file, machines_file))\n",
    "        self.max_subgraph_size: int = max_subgraph_size\n",
    "        self.gurobi_factory: SolverFactory = GurobiSolverFactory(self.tasks_file, self.machines_file, self.deadline)\n",
    "        self.cqm_factory: SolverFactory = CQMSolverFactory(self.tasks_file, self.machines_file, self.deadline)\n",
    "\n",
    "    def run(self) -> CQMExperimentResult:\n",
    "        gurobi_run = AlgorithmRun(\n",
    "            self.max_subgraph_size, \n",
    "            self.gurobi_factory.get_decomposed_solver(self.max_subgraph_size).solve(), \n",
    "            self.gurobi_factory.get_reference_solver().solve()\n",
    "        )\n",
    "        # Enable CQM for final experiment run\n",
    "        # cqm_run = AlgorithmRun(\n",
    "        #     self.max_subgraph_size,\n",
    "        #     self.cqm_factory.get_decomposed_solver(self.max_subgraph_size).solve(),\n",
    "        #     None\n",
    "        # )\n",
    "        cqm_run = None\n",
    "        return self.CQMExperimentResult(\n",
    "            gurobi_run, \n",
    "            cqm_run,\n",
    "            self.tasks_file,\n",
    "            self.machines_file,\n",
    "            self.deadline,\n",
    "            self.max_subgraph_size\n",
    "        )\n",
    "\n",
    "def deadline_as_cpv(tasks_file, machines_file):\n",
    "    workflow = Workflow(tasks_file, machines_file, 100000)\n",
    "    mean_times = workflow.time_matrix.mean(axis=1).to_dict()\n",
    "\n",
    "    def path_load(p):\n",
    "        return sum([mean_times[t] for t in p])\n",
    "\n",
    "    return int(max(path_load(p) for p in workflow.paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a106f39-a8dd-4482-b6fc-f3539000506d",
   "metadata": {},
   "source": [
    "# Actual experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65cf13-8e77-4338-b36e-eb39ff3bb7c8",
   "metadata": {},
   "source": [
    "### Montage 310 nodes, mss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0edff-6044-42d9-9184-1028367969ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "montage_310 = f\"{PEGASUS_INSTANCES_DIR}/montage/chameleon-cloud/montage-chameleon-2mass-015d-001.json\"\n",
    "machines_file = \"../resources/machines/linear_smaller_diff.json\"\n",
    "max_subgraph_size = 100\n",
    "\n",
    "experiment = CQMDecompositionExperiment(montage_310, machines_file, max_subgraph_size)\n",
    "experiment_result = experiment.run()\n",
    "experiment_result.save_execution_report(save_dir = \"./montage_310_100\", file_prefix = \"montage_310_mss_100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ffdb1-18a4-4d79-9edd-abf4bb006631",
   "metadata": {},
   "source": [
    "### Montage 472, mss = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc9cac-22d4-46a5-b055-119c03051aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "montage_472 = f\"{PEGASUS_INSTANCES_DIR}/montage/chameleon-cloud/montage-chameleon-dss-10d-001.json\"\n",
    "machines_file = \"../resources/machines/linear_smaller_diff.json\"\n",
    "max_subgraph_size = 150\n",
    "\n",
    "experiment = CQMDecompositionExperiment(montage_472, machines_file, max_subgraph_size)\n",
    "experiment_result = experiment.run()\n",
    "experiment_result.save_execution_report(save_dir = \"./montage_472_150\", file_prefix = \"montage_472_mss_150\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06c06e-db64-4c07-87d6-3d0af15e12e2",
   "metadata": {},
   "source": [
    "### Montage 619, mss = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d75b4-303b-46dd-b8d0-6cdb71f44928",
   "metadata": {},
   "outputs": [],
   "source": [
    "montage_619 = f\"{PEGASUS_INSTANCES_DIR}/montage/chameleon-cloud/montage-chameleon-2mass-025d-001.json\"\n",
    "machines_file = \"../resources/machines/linear_smaller_diff.json\"\n",
    "max_subgraph_size = 200\n",
    "\n",
    "experiment = CQMDecompositionExperiment(montage_619, machines_file, max_subgraph_size)\n",
    "experiment_result = experiment.run()\n",
    "experiment_result.save_execution_report(save_dir = \"./montage_619_200\", file_prefix = \"montage_619_mss_200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b218c-5e46-4748-9278-628eb5d22484",
   "metadata": {},
   "source": [
    "### Montage 1066 nodes, mss = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24071a-996c-4ec2-86be-e2681f8adf23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "montage_1066 = f\"{PEGASUS_INSTANCES_DIR}/montage/chameleon-cloud/montage-chameleon-dss-125d-001.json\"\n",
    "machines_file = \"../resources/machines/linear_smaller_diff.json\"\n",
    "max_subgraph_size = 350\n",
    "\n",
    "experiment = CQMDecompositionExperiment(montage_1066, machines_file, max_subgraph_size)\n",
    "experiment_result = experiment.run()\n",
    "experiment_result.save_execution_report(save_dir = \"./montage_1066_350\", file_prefix = \"montage_1066_mss_350\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c242c5a-2e36-4c0d-8666-711e6dfee676",
   "metadata": {},
   "source": [
    "Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a7a014-dc92-4164-9ca5-f43b940fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_small = f\"{PEGASUS_INSTANCES_DIR}/1000genome/chameleon-cloud/1000genome-chameleon-2ch-250k-001.json\"\n",
    "machines_file = \"../resources/machines/linear_smaller_diff.json\"\n",
    "max_subgraph_size = 30\n",
    "\n",
    "experiment = CQMDecompositionExperiment(genome_small, machines_file, max_subgraph_size)\n",
    "experiment_result = experiment.run()\n",
    "experiment_result.save_execution_report(save_dir = \"./genome\", file_prefix = \"genome_small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
